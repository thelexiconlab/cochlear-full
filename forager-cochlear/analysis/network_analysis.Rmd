---
title: "Cochlear Analysis"
author: "Channing Hambric"
date: "2024-09-30"
output:
  html_document: default
  pdf_document: default
---

###THIS STILL NEEDS TO BE CLEANED UP


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Libraries
```{r}
library(tidyverse)
library(ggthemes)
library(igraph)
```
# import data
```{r}

#For assigning CI/NH status
participant_groupings <- read.csv("../participant_groupings.csv")

#using best fit structure for each group
semantics<-read.csv("best_indv_model_lexical.csv") %>% select(Subject,Group,domain,Fluency_Item,structure,semantic,phonological,frequency,alpha)

semantics_overall<-semantics%>%
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological),alpha_recoded = ifelse(alpha > 0.4, "high alpha", "low alpha"))

word2vec_nh_animals<-semantics %>%filter(Group=="NH",structure=="word2vec",domain=="animals")%>%
  rename(word2vec = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological))

word2vec_ci_animals<-semantics %>%filter(Group=="CI",structure=="word2vec",domain=="animals")%>%
  rename(word2vec = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological))



speech2vec_nh_animals<-semantics %>%filter(Group=="NH",structure=="speech2vec",domain=="animals")%>%
  rename(speech2vec = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological)) 

speech2vec_ci_animals<-semantics %>%filter(Group=="CI",structure=="speech2vec",domain=="animals")%>%
  rename(speech2vec = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological))



blended_nh_animals<-semantics %>%filter(Group=="NH",structure=="blended",domain=="animals")%>%
  rename(blended = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological),alpha_recoded = ifelse(alpha > 0.4, "high alpha", "low alpha"))

blended_ci_animals<-semantics %>%filter(Group=="CI",structure=="blended",domain=="animals")%>%
  rename(blended = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological),alpha_recoded = ifelse(alpha > 0.4, "high alpha", "low alpha"))



blended_nh_foods<-semantics %>%filter(Group=="NH",structure=="blended",domain=="foods")%>%
  rename(blended = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological),alpha_recoded = ifelse(alpha > 0.4, "high alpha", "low alpha"))

blended_ci_foods<-semantics %>%filter(Group=="CI",structure=="blended",domain=="foods")%>%
  rename(blended = semantic) %>% 
  group_by(Subject) %>% mutate(item = row_number()) %>%
  left_join(participant_groupings) %>%
  mutate(Frequency_Value = frequency/100,
         Phonological_Similarity = ifelse(phonological == 0.0001, 0, phonological),alpha_recoded = ifelse(alpha > 0.4, "high alpha", "low alpha"))


```
# create edges

```{r}
library(igraph)
library(dplyr)

# Function to create a well-formatted network graph
create_graph <- function(df, group_label, similarity_metric) {
  
   # Filter for the given group
  edges <- df %>%
    filter(Group == group_label) %>%
    group_by(Subject) %>%
    mutate(Next_Word = lag(Fluency_Item)) %>%
    na.omit() %>%
    ungroup() %>%
    transmute(From = Fluency_Item, 
              To = Next_Word, 
              Weight = .data[[similarity_metric]]) %>%
    filter(Weight > 0) %>% distinct()
  
  # Create graph
  g <- graph_from_data_frame(edges, directed = FALSE)
  
  # Compute network metrics
  avg_path_length <- mean_distance(g, directed = FALSE, unconnected = TRUE)  # Handle disconnected components
  clustering_coeff <- transitivity(g, type = "global")  # Global clustering coefficient
  graph_density <- edge_density(g)  # Density of the graph
  degree_centrality <- degree(g)  # Number of connections per node
  
  # Print network metrics
  cat("\nNetwork Metrics for Group:", group_label, "(", similarity_metric, ")\n")
  cat("---------------------------------------------------\n")
  cat("Average Shortest Path Length:", round(avg_path_length, 3), "\n")
  cat("Clustering Coefficient:", round(clustering_coeff, 3), "\n")
  cat("Graph Density:", round(graph_density, 3), "\n")
  cat("Top 5 Most Central Nodes (by Degree):\n")
  print(sort(degree_centrality, decreasing = TRUE)[1:5])

  # Improve layout
  set.seed(42)  # Ensure consistent layout
  layout <- layout_with_fr(g)  # Force-directed layout

  # Plot graph with improved aesthetics
  plot(g, 
       layout = layout,
       edge.width = E(g)$Weight * 3,  # Scale edge width
       vertex.label.cex = 0.7,  # Reduce label size
       vertex.label.dist = 1,  # Push labels away from nodes
       vertex.color = ifelse(group_label == "NH", "skyblue", "forestgreen"),  # Color nodes by group
       vertex.frame.color = "gray30",
       edge.color = "gray70",
       vertex.size = 2,
       main = paste("Network - Group", group_label, similarity_metric))
}

# Example usage:
#create_graph(blended_nh_animals, "NH", "blended")
```


```{r}

#across all structural models
#animals
create_graph(semantics_overall%>%filter(Group=="NH",domain=="animals"), "NH", "semantic")
create_graph(semantics_overall%>%filter(Group=="CI",domain=="animals"), "CI", "semantic")
#longer ASPL for CI, also slightly more dense

#foods
create_graph(semantics_overall%>%filter(Group=="NH",domain=="foods"), "NH", "semantic")
create_graph(semantics_overall%>%filter(Group=="CI",domain=="foods"), "CI", "semantic")
#longer ASPL for CI, higher clustering coefficient and density for NH

#phonsim
#animals
create_graph(semantics_overall%>%filter(Group=="NH",domain=="animals"), "NH", "Phonological_Similarity")
create_graph(semantics_overall%>%filter(Group=="CI",domain=="animals"), "CI", "Phonological_Similarity")
#longer ASPL for CI, higher clustering coefficient NH, density about the same

#foods
create_graph(semantics_overall%>%filter(Group=="NH",domain=="foods"), "NH", "Phonological_Similarity")
create_graph(semantics_overall%>%filter(Group=="CI",domain=="foods"), "CI", "Phonological_Similarity")
#higher clustering coefficient for NH, everything else about the same


#alpha group counts
semantics_overall %>% filter(structure=="blended")%>%
  group_by(Group,domain,alpha_recoded) %>%
  summarise(Unique_Subjects = n_distinct(Subject))



#looking at graph density by alpha - is weighting more s2v assc with a more or less connected network
#animals
create_graph(blended_nh_animals %>%filter(alpha_recoded=="low alpha"), "NH", "blended")
create_graph(blended_nh_animals %>%filter(alpha_recoded=="high alpha"), "NH", "blended")
#for NHs, longer ASPL for low alphas, higher clustering coefficient and density for higher alphas (more s2v)

create_graph(blended_ci_animals %>%filter(alpha_recoded=="low alpha"), "CI", "blended")
create_graph(blended_ci_animals %>%filter(alpha_recoded=="high alpha"), "CI", "blended")
#for CIs,  longer ASPL for low alphas, higher clustering coefficient for higher alphas, density about the same

#foods
create_graph(blended_nh_foods %>%filter(alpha_recoded=="low alpha"), "NH", "blended")
create_graph(blended_nh_foods %>%filter(alpha_recoded=="high alpha"), "NH", "blended")
#for NHs, larger ASPL, less clustered and less dense for high alphas

create_graph(blended_ci_foods %>%filter(alpha_recoded=="low alpha"), "CI", "blended")
create_graph(blended_ci_foods %>%filter(alpha_recoded=="high alpha"), "CI", "blended")
#for CIs, larger ASPL and density for low alphas



#looking at phon sim

#animals
create_graph(blended_nh_animals, "NH", "Phonological_Similarity")
create_graph(blended_ci_animals, "CI", "Phonological_Similarity")
#larger ASPL for CIs, higher clustering coefficient for NH, density about the same

#foods
create_graph(blended_nh_foods, "NH", "Phonological_Similarity")
create_graph(blended_ci_foods, "CI", "Phonological_Similarity")
#larger ASPL for CIs, much higher clustering coefficient for NH, CIs slightly more dense

#comparing alphas separately
#animals
create_graph(blended_nh_animals %>%filter(alpha_recoded=="low alpha"), "NH", "Phonological_Similarity")
create_graph(blended_nh_animals %>%filter(alpha_recoded=="high alpha"), "NH", "Phonological_Similarity")
#larger clustering coefficient and ASPL for high alpha, but low alpha slightly more dense

create_graph(blended_ci_animals %>%filter(alpha_recoded=="low alpha"), "CI", "Phonological_Similarity")
create_graph(blended_ci_animals %>%filter(alpha_recoded=="high alpha"), "CI", "Phonological_Similarity")
#larger ASPL for high alpha, much higher CC, but low alpha more dense

#foods
create_graph(blended_nh_foods %>%filter(alpha_recoded=="low alpha"), "NH", "Phonological_Similarity")
create_graph(blended_nh_foods %>%filter(alpha_recoded=="high alpha"), "NH", "Phonological_Similarity")
#larger ASPL for high alphas, but less clustered and less dense than low alphas

create_graph(blended_ci_foods %>%filter(alpha_recoded=="low alpha"), "CI", "Phonological_Similarity")
create_graph(blended_ci_foods %>%filter(alpha_recoded=="high alpha"), "CI", "Phonological_Similarity")
#larger ASPL for high alpha, higher CC, but low alpha more dense
```


