---
title: "Cochlear Analysis"
author: "Channing Hambric"
date: "2025-02-10"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set up 
## Libraries
```{r}
library(tidyverse)
library(ggthemes)
library(purrr)
library(ggpubr)
library(lubridate)
library(dplyr)
library(ggplot2)
library(tidyboot)
library(gridExtra)
library(lme4)
library(emmeans)
library(data.table)
library(knitr)
library(kableExtra)
library(uuid)
library(paletteer)
library(ggh4x)
library(geomtextpath)
library(ggbeeswarm)
library(rcartocolor)
library(patchwork)
library(ggridges)
library(forcats)
library(broom)
library(data.table)
library(ggstats)
library(scales)
```

## Define Data Import & Validation Functions

```{r}

domains <- c("animals", "foods")  
models <- c("word2vec", "speech2vec", "USE", "blended")
dimensions <- c("50", "100", "200", "300", "512")
alphas <- c("0.0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0")

# Function to read and process files
read_model_data <- function(file_type, domain, model, dimension, alpha) {
  valid_file_types <- c("lexical_results.csv", "model_results.csv", 
                        "individual_descriptive_stats.csv", "switch_results.csv")
  
  if (!(file_type %in% valid_file_types)) {
    stop(sprintf("Invalid file_type: %s. Choose from: %s", file_type, paste(valid_file_types, collapse = ", ")))
  }
  
  file_path <- sprintf("../output/%s/%s/%s/%s/%s", domain, model, dimension, alpha, file_type)
  
  if (file.exists(file_path)) {
    read.csv(file_path) %>%
      mutate(domain = domain,      
             dimension = as.character(dimension),
             structure = model,
             alpha = alpha,
             file_type = file_type)  # Track which file type this row comes from
  } else {
    NULL
  }
}

# Function to combine multiple datasets based on file type
combine_model_data <- function(file_type) {
  map_dfr(domains, function(domain) {
    map_dfr(models, function(model) {
      map_dfr(dimensions, function(dimension) {
        map_dfr(alphas, function(alpha) {
          read_model_data(file_type, domain, model, dimension, alpha)
        })
      })
    })
  })
}

# Function to validate row counts by structure, dimension, and alpha for each domain
validate_combined_data <- function(combined_data, file_type) {
  # Load individual domain datasets
  animals_data <- combined_data %>% filter(domain == "animals")
  foods_data <- combined_data %>% filter(domain == "foods")
  
  # Group and count rows for each combination of structure, dimension, and alpha
  animals_counts <- animals_data %>% count(structure, dimension, alpha, name = "count")
  foods_counts <- foods_data %>% count(structure, dimension, alpha, name = "count")
  
  # Check if counts are equal within each domain
  animals_valid <- all(animals_counts$count == animals_counts$count[1])
  foods_valid <- all(foods_counts$count == foods_counts$count[1])
  
  # Stop execution if counts are inconsistent within domains
  if (!animals_valid) {
    print("❌ Row count mismatch in 'animals' data for some combinations:")
    print(animals_counts)
    stop("Animals data has inconsistent row counts across structure-dimension-alpha combinations.")
  }
  
  if (!foods_valid) {
    print("❌ Row count mismatch in 'foods' data for some combinations:")
    print(foods_counts)
    stop("Foods data has inconsistent row counts across structure-dimension-alpha combinations.")
  }
  
  # Total row count validation
  total_rows_check <- (nrow(animals_data) + nrow(foods_data)) == nrow(combined_data)
  
  if (!total_rows_check) {
    stop("❌ Total row count mismatch: Sum of individual domain rows does not match the combined dataset.")
  }
  
  message(sprintf("✅ Validity check passed for %s: Row counts are consistent within each domain and total sum matches!", file_type))
}


```

## Import & Restructure Data

```{r}
#For assigning CI/NH status 
participant_groupings <- read.csv("../participant_groupings.csv")

#import data
#lexical results
full_lexical_data <- combine_model_data("lexical_results.csv") %>%select(-file_type)%>%left_join(participant_groupings)

#check_data
validate_combined_data(full_lexical_data, "lexical_results.csv")

#indv descrp stats
#takes a few sec
full_indv_data <- combine_model_data("individual_descriptive_stats.csv") %>%select(-file_type)%>%left_join(participant_groupings) %>% rename("fluency_score"="X._of_Items")%>%separate(Switch_Method, 
           into = c("method", "param1", "param2", "param3"), #separating out parameters
           sep = "_", 
           fill = "right")%>%mutate(full_method=paste(method,param1,param2,param3,sep="_"))
#check data
validate_combined_data(full_indv_data, "individual_descriptive_stats.csv")

#model results
#Takes a few min
full_model_data <- combine_model_data("model_results.csv") %>%select(-file_type)%>%left_join(participant_groupings) %>%
  separate(Model, 
           into = c("forage", "foraging_type", "method", "param1", "param2", "param3"), #separating out model parameters
           sep = "_", 
           fill = "right") %>%
  mutate(
    forage = factor(forage),
    foraging_type = factor(foraging_type),
    method = factor(method),
    param1 = factor(param1),
    param2 = factor(param2),
    param3 = factor(param3),
    dimension = factor(dimension),
    alpha = factor(alpha))%>%
  mutate(model_type = fct_recode(foraging_type, 
                                  `pstatic` = "phonologicalstatic", #renaming models
                                  `plocal` = "phonologicaldynamiclocal",
                                  `pglobal` = "phonologicaldynamicglobal",
                                  `pswitch` = "phonologicaldynamicswitch",
                                  `static` = "static", 
                                  `dynamic` = "dynamic", 
                                  `random` = "random")) %>%
  mutate(full_method=paste(method,param1,param2,param3,sep="_"))%>%
  mutate(full_model=paste(model_type,full_method,sep="_"))%>%
    mutate(full_speech=paste(structure,dimension,alpha,sep="_"))%>%
  mutate(everything=paste(full_model,full_speech,sep="_"))

#check data
validate_combined_data(full_model_data, "model_results.csv")

#switch designations
#A LOT OF DATA - using data.table
full_switch_data <- combine_model_data("switch_results.csv")%>%select(-file_type)
#Convert to data.table
setDT(full_switch_data)
setDT(participant_groupings)
#join with labels
full_switch_data <- participant_groupings[full_switch_data, on = "Subject"]  
#separate params
full_switch_data[, c("method", "param1", "param2", "param3") := tstrsplit(Switch_Method, "_", fixed = TRUE, fill = "NA")]
full_switch_data[, full_method := paste(method, param1, param2, param3, sep = "_")]
#convert back
full_switch_data<-as.data.frame(full_switch_data)
#check data
validate_combined_data(full_switch_data, "switch_results.csv")

```
# Analysis Starts Here

# Model Analyses

```{r}

#excluding USE
#only using multimodal delta and norms switch methods

#Getting N
just_fluency<-full_indv_data %>%select(Subject,Group,domain,fluency_score) %>%rename("N"="fluency_score") %>%na.omit() %>%unique()

#Extract random model's nLL for each Subject x domain x structure x dim x alpha
random_nLL_lookup <- full_model_data %>%
  group_by(Subject, domain, Group, structure, dimension, alpha) %>%
  filter(model_type == "random") %>%
  mutate(random_k = ifelse(structure == "blended", 2, 1)) %>% #setting random model k's
  select(Subject, domain, Group, structure, dimension, alpha, random_k, 
         random_nLLs = Negative_Log_Likelihood_Optimized)


#setting up parameter matching
full_model_data_parameters<-full_model_data %>%mutate(parameter_link=paste(model_type,method,structure,sep="_"))%>% filter(!(Subject %in% c("CAN-667","SFZ-149","SOA-317","CDH-754","SGO-159"))) #exclude missing demos and atypical subjs

#importing parameter counts
parameters<-read.csv("Cochlear_BIC_Parameters.csv") %>%select(parameter_link,k)

#calculating BIC
bic <- full_model_data_parameters %>%
  left_join(just_fluency, by = c("Subject", "Group", "domain")) %>%
  left_join(random_nLL_lookup, by = c("Subject","domain","Group","structure","dimension","alpha")) %>%  
  left_join(parameters, by="parameter_link") %>%
  mutate(
    optimalBIC = k * log(N) - 2 * (-Negative_Log_Likelihood_Optimized),
    randomBIC = random_k * log(N) - 2 * (-random_nLLs)) 

#dropping unused structures and switch methods
bic_clean<-bic %>%filter(structure != "USE", !(method %in% c("simdrop", "delta", "multimodal")),model_type!="random")

#Including word2vec, speech2vec & blended:

#best delta BIC model per participant
#best_delta_bic_all_struct = bic_clean%>% 
 # group_by(Subject,Group,domain) %>%
 # mutate(deltaBIC  = randomBIC - optimalBIC)%>%
 # arrange(desc(deltaBIC)) %>%
 # slice(1)

#participants with single model as best fit 
#single_model_best<-best_delta_bic_all_struct %>% filter(structure %in% c("word2vec","speech2vec")) %>%select(Subject,Group,domain,structure)

#best delta BIC model per participant for just blended structure **Use this**
best_blended_delta_bic = bic_clean%>% filter(structure=="blended")%>%
  group_by(Subject,Group,domain) %>%
  mutate(deltaBIC  = randomBIC - optimalBIC)%>%
  arrange(desc(deltaBIC)) %>%
  slice(1)

#checking blended alphas for those who previously had a single model best fit
#check_alphas<-single_model_best %>%left_join(best_blended_delta_bic %>% select(Subject,Group, domain, alpha))

#check_alphas %>%
 # group_by(structure,alpha) %>%
 # count() %>%
#  mutate(prop = n / sum(n)) %>%  #compute proportion 
 # ggplot(aes(x = structure, y = prop, fill = alpha)) +
 # geom_bar(stat = "identity", position = "fill") +  
 # geom_text(aes(label = percent(prop, accuracy = 0.1)), 
 #           position = position_fill(vjust = 0.5)) + 
 # labs(y = "proportion", x="best-fit single structure",fill = "best-fit blended alpha", 
  #     title = "") +
  #theme_few()+scale_fill_paletteer_d("rcartocolor::Pastel")


#Best models per Group x domain
#median delta BIC
#median_delta_bic = bic_clean %>%filter(structure=="blended")%>%
 # mutate(deltaBIC  = randomBIC - optimalBIC) %>%
 # group_by(Group,domain,everything) %>%
 # summarise(mediandeltaBIC = median(deltaBIC)) %>%
 # arrange(desc(mediandeltaBIC)) %>%
 # slice(1) %>% 
 # ungroup() 
#kable(median_delta_bic)

#Getting subject counts
best_blended_delta_bic %>%
  group_by(Group, domain) %>%
  summarise(Unique_Subjects = n_distinct(Subject), .groups = "drop")


```

## Looking at Individual Differences
```{r}
#set up df
#make sure alphas are numeric
best_blended_delta_bic_analysis <- best_blended_delta_bic %>%
  mutate(
    Group = factor(Group, levels = c("NH", "CI")),
    dimension = factor(dimension, levels = c("50", "100", "200", "300")),
    param1 = ifelse(param1 %in% c("categorical", "associative"), NA_real_, 
                    as.numeric(gsub("alpha=", "", param1))),
    alpha = as.numeric(as.character(alpha)),
    method = as.character(method),model_type = as.character(model_type))

#CHI SQUARE ANALYSES

#Chi square for methods
method_table <- table(best_blended_delta_bic_analysis$domain, best_blended_delta_bic_analysis$method)
method_result <- chisq.test(method_table)
print(method_result)

#multimodal significantly more common for foods


#chi square for models
model_table <- table(best_blended_delta_bic_analysis$Group, best_blended_delta_bic_analysis$model_type)

#static
model_table_subset_static <- model_table[,"static"]
model_result_static <- chisq.test(model_table_subset_static)
print(model_result_static)

#CIs sig more likely to have static model

#pswitch
model_table_subset_pswitch <- model_table[,"pswitch"]
model_result_pswitch <- chisq.test(model_table_subset_pswitch)
print(model_result_pswitch)

#NHs sig more likely to have pswitch model


#Relationship between structural elements and fluency?
#ns fx of dimension, structural alpha, or multimodal alpha


```
 
## Individual Differences Plots
```{r}

#Looking at dimension
best_blended_delta_bic_analysis%>%
  group_by(Group, domain, dimension) %>%
  count() %>%
  group_by(Group, domain) %>%  #calculate within each group x domain
  mutate(prop = n / sum(n)) %>%  #compute proportion 
  ggplot(aes(x = Group, y = prop, fill = dimension)) +
  geom_bar(stat = "identity", position = "fill") +  
  geom_text(aes(label = percent(prop, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5)) + 
  facet_wrap(~domain) +  
  labs(y = "proportion", fill = "dimension", 
       title = "") +
  theme_few()+scale_fill_paletteer_d("rcartocolor::Pastel")

#Looking at structural alphas
best_blended_delta_bic_analysis%>% mutate(alpha=as.character(alpha))%>%
  group_by(Group, domain, alpha) %>%
  count() %>%
  group_by(Group, domain) %>%  #calculate within each group x domain
  mutate(prop = n / sum(n)) %>%  #compute proportion 
  ggplot(aes(x = Group, y = prop, fill = alpha)) +
  geom_bar(stat = "identity", position = "fill") +  
  geom_text(aes(label = percent(prop, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5)) + 
  facet_wrap(~domain) +  
  labs(y = "proportion", fill = "structural alpha", 
       title = "") +
  theme_few()+scale_fill_paletteer_d("colorBlindness::Blue2Orange12Steps")
  
#Switch methods
#excluding those with static best fit
best_blended_delta_bic_analysis%>% filter(!(model_type %in% c("static","pstatic")))%>%
  group_by(Group, domain, method) %>% 
  count() %>%
  group_by(Group, domain) %>%  #calculate within each group x domain
  mutate(prop = n / sum(n)) %>%  #compute proportion 
  ggplot(aes(x = Group, y = prop, fill = method)) +
  geom_bar(stat = "identity", position = "fill") +  
  geom_text(aes(label = percent(prop, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5)) + 
  facet_wrap(~domain) +  
  labs(y = "proportion", fill = "switch method", 
       title = "") +
  theme_few()+scale_fill_paletteer_d("rcartocolor::Pastel")

#multimodaldelta alphas for those with multimodaldelta best method
best_blended_delta_bic_analysis%>% filter(method =="multimodaldelta")%>%mutate(param1=as.character(param1))%>%
  group_by(Group, domain, param1) %>%
  count() %>%
  group_by(Group, domain) %>%  #calculate within each group x domain
  mutate(prop = n / sum(n)) %>%  #compute proportion 
  ggplot(aes(x = Group, y = prop, fill =param1)) +
  geom_bar(stat = "identity", position = "fill") +  
  geom_text(aes(label = percent(prop, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5)) + 
  facet_wrap(~domain) +  
  labs(y = "proportion", fill = "multimodal alpha", 
       title = "") +
  theme_few()+scale_fill_paletteer_d("colorBlindness::Blue2Orange12Steps")
  
#best model
best_blended_delta_bic_analysis%>%
    group_by(Group,domain,model_type) %>%
  count() %>%
  group_by(Group, domain) %>%  #calculate within each group x domain
  mutate(prop = n / sum(n)) %>%  #compute proportion 
  ggplot(aes(x = Group, y = prop, fill =model_type)) +
  geom_bar(stat = "identity", position = "fill") +  
  geom_text(aes(label = percent(prop, accuracy = 0.1)), 
            position = position_fill(vjust = 0.5)) + 
  facet_wrap(~domain) +  
  labs(y = "proportion", fill = "model", 
       title = "") +
  theme_few()+scale_fill_paletteer_d("rcartocolor::Pastel")


#structural alpha x fluency
ggplot(best_blended_delta_bic_analysis, aes(x = alpha, y = N, group=Group,color=Group)) +  geom_point()+facet_grid(~domain)+
  geom_smooth(method = "lm", se = FALSE) +  
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +  
  theme_few()+labs(y = "fluency score", x = "structural alpha")+scale_colour_paletteer_d("nationalparkcolors::CraterLake")


#multiumodal alpha x fluency
ggplot(best_blended_delta_bic_analysis, aes(x = param1, y = N, group=Group,color=Group)) +  geom_point()+facet_grid(~domain)+
  geom_smooth(method = "lm", se = FALSE) +  
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +  
  theme_few()+labs(y = "fluency score", x = "multimodal alpha")+scale_colour_paletteer_d("nationalparkcolors::CraterLake")

```

## Lexical Analyses
```{r}
#setting up phonology and frequency df
lexical_analysis_data <-full_indv_data %>% select(Subject, Group,Phonological_Similarity_mean, Frequency_Value_mean, 
         fluency_score, domain) %>% na.omit() %>% left_join(best_blended_delta_bic %>% select(Subject,domain,Group,alpha)) %>%mutate(alpha=as.numeric(as.character(alpha)))%>%
  rename("phonological" = "Phonological_Similarity_mean",
         "frequency" = "Frequency_Value_mean")%>%
  pivot_longer(names_to = "cue", cols=c(phonological,frequency))%>%mutate(Group = factor(Group, levels = c("NH","CI"))) %>% unique()%>%
  filter(!(Subject %in% c("CAN-667","SFZ-149","SOA-317","CDH-754","SGO-159"))) #removing participants within missing demo info

#setting up the semantic df
#using each participants best semantic structure
lexical_analysis_data_semantic<-best_blended_delta_bic_analysis%>%
  left_join(full_indv_data%>%mutate(alpha=as.numeric(alpha)) %>%select(Subject,Group,domain,structure,dimension,alpha,Semantic_Similarity_mean)%>%na.omit())%>% mutate(Group = factor(Group, levels = c("NH", "CI")))


#Does fluency score differ by Group and domain
fluency_lm = lm(data = lexical_analysis_data, fluency_score ~ Group*domain)
summary(fluency_lm)
car::Anova(fluency_lm)

#SEMANTIC ANALYSES 
#Using everyones best-fit structure, does avg semantic similarity differ by Group x domain x structural alpha, fluency score as covariate
semantic_model<- lm(Semantic_Similarity_mean ~ Group * domain * N * alpha, data = lexical_analysis_data_semantic)
summary(semantic_model)
car::Anova(semantic_model)
#everything ns

#no sig fx when looking at dimension either (looked at separately since not equal representation of alphas across dimensions)


#PHONOLOGY & FREQUENCY ANALYSES

#note these values are the same across model variations
#Does phonological similarity and frequency differ across Group x domain, fluency score as covariate
# Run lm() for each cue and extract results
lexical_results <- lexical_analysis_data%>%
  group_by(cue) %>%                                   # Group by cue
  nest() %>%                                         # Nest data by cue
 mutate(
    model = map(data, ~ lm(value ~ Group * domain * fluency_score, data = .)) # Fit models
  )
#View results
model_summaries <- lexical_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(model_summaries)

#only sig effect was fluency score for freq, where those who produced less words produced more frequent words

#No lexical metrics sig predicted fluency


##ALPHA ANALYSES

#relationship between alphas and phonological similarity
#nothing significant with structural alpha
struct_alpha_model<-lm(data = phon_struct_data, value ~ alpha*Group*domain*fluency_score)
summary(struct_alpha_model)

#multimodal alpha
#set up df
phon_multi_data<-lexical_analysis_data %>% left_join(best_blended_delta_bic %>% select(Subject,domain,Group,method,param1))%>%
  filter(method=="multimodaldelta",cue=="phonological")%>%mutate(Group = factor(Group, levels = c("NH","CI"))) %>%
  mutate(param1 = as.numeric(gsub("alpha=", "", param1)))

multi_alpha_model<-lm(data = phon_multi_data, value ~ param1*Group*domain*fluency_score)
summary(multi_alpha_model)

#sig intx group x domain x fluency
#sig intx of group x domain x multimodal alpha

#seeing if multimodal alpha is significant for just NH
multi_alpha_model_nh<-lm(data = phon_multi_data%>%filter(Group=="NH"), value ~ param1*domain*fluency_score)
summary(multi_alpha_model_nh)
#its not

```

### Fluency plot
```{r}
fluency_plot <- lexical_analysis_data %>%
  ggplot(aes(x = domain, y = fluency_score, fill = Group)) +
  geom_boxplot(color = "black") +
  ylim(0, 50) +  
  labs(y = 'Fluency score', x = "Domain", title = "") +
  theme_few() +
  theme(
    aspect.ratio = 1, axis.text.y = element_text(size = 14),axis.text.x = element_text(size = 14),
    legend.position = 'right',
    plot.title = element_text(size = 18)  
  ) +
  scale_fill_paletteer_d("nationalparkcolors::CraterLake") 

fluency_plot

```

### Lexical plots
```{r}
#semantic plots

semantic_plot <- emmip(semantic_model, Group ~ N | domain,  
      cov.reduce = range, CI = TRUE) +
  labs(title = "",
       x = "Fluency Score", 
       y = "Estimated Semantic Similarity") +
  scale_colour_paletteer_d("nationalparkcolors::CraterLake") +
  theme_few() +
  theme(aspect.ratio = 1.0, legend.position = "top")

semantic_plot


#phonological and frequency plots
plots_lexical <- lexical_results %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ fluency_score | domain,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "", x = "Fluency score", y = paste("Estimated", unique(cue))) +facet_wrap(~ domain) 
      +scale_colour_paletteer_d("nationalparkcolors::CraterLake") +
      theme_few() + 
      theme(aspect.ratio = 1.5, legend.position = "top")))

semantic_plot+plots_lexical$emmip_plot[[1]]+plots_lexical$emmip_plot[[2]]

#multimodal alpha x phon similarity
ggplot(phon_multi_data, aes(x = param1, y = value, group=Group,color=Group)) +  geom_point()+facet_grid(~domain)+
  geom_smooth(method = "lm", se = TRUE) +  
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +  
  theme_few()+labs(y = "avg phonological similarity", x = "multimodal alpha")+scale_colour_paletteer_d("nationalparkcolors::CraterLake")

#limited data in some of these groups --eg only 5 NH-foods w/ a high alpha, may not be interpretable - maybe exclude norms?
```


## Salience Parameter (Beta) Analyses
```{r}
#using participant best models
#Separating out betas
beta_values =best_blended_delta_bic  %>%
  pivot_longer(names_to = "beta", cols = c(Beta_Frequency, Beta_Phonological, Beta_Semantic))%>%mutate(Group = factor(Group, levels = c("NH","CI")))


# Run lm() for each beta type and extract results
beta_results <- beta_values %>%
  group_by(beta) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group*domain, data = .)) 
  )

#View results
beta_summaries <- beta_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(beta_summaries)

#only fx for semantics - sig lower similarity for foods, sig intx between group x domain, CIs higher foods semantic beta

```

### Beta plot

```{r}
beta_values %>%mutate(value = replace_na(value, 0))%>%
  group_by(Group, domain, beta) %>%
  tidyboot_mean(value, nboot = 1000, na.rm = TRUE) %>%
  separate(beta, into = c("b", "beta")) %>%
  mutate(
    beta = tolower(beta),
    beta = fct_recode(beta, 
                      `Semantic\nSimilarity` = "semantic", 
                      `Phonological\nSimilarity` = "phonological",
                      `Word\nFrequency` = "frequency")
  ) %>%
  mutate(beta = fct_relevel(beta, "Semantic\nSimilarity", "Phonological\nSimilarity", "Word\nFrequency")) %>%
  ggplot(aes(x = beta, y = empirical_stat, group = Group, fill = Group)) + facet_wrap(~domain)+
  geom_bar(stat = 'identity', position = "dodge") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = position_dodge(0.9)) +
  labs(y = bquote(beta ~ "(Parameter Salience)"), x = "") +
  theme_few() +
  theme() +
  scale_fill_paletteer_d("nationalparkcolors::CraterLake")
```

## Clustering Analyses
```{r}
#filtering to only include indv best from above
best_clusters <- full_indv_data %>%mutate(Group = factor(Group, levels = c("NH","CI")))%>%
  filter(paste(Subject,Group,domain,full_method,dimension,structure,alpha) %in% paste(best_blended_delta_bic$Subject,best_blended_delta_bic$Group,best_blended_delta_bic$domain, best_blended_delta_bic$full_method,best_blended_delta_bic$dimension,best_blended_delta_bic$structure,best_blended_delta_bic$alpha)) %>%select(-fluency_score)%>% #this is full of NAs & produces mismatch .x/.y error, so dropping
  group_by(Subject,Group,domain) %>%left_join(lexical_analysis_data%>%select(Subject,Group,domain,fluency_score)%>%unique(), by = c("Subject", "Group","domain")) #adding fluency_score back in

#avg cluster size
#number of items produced as a covariate
domain_cluster_size_lm = lm(data = best_clusters, Cluster_Size_mean ~ Group * domain * fluency_score)
summary(domain_cluster_size_lm)
car::Anova(domain_cluster_size_lm)
emmeans(domain_cluster_size_lm,pairwise~Group*domain,simple="Group")

#marginal effect of domain and marg intx of domain x fluency score

#number of switches
domain_switch_numb_lm = lm(data = best_clusters, Number_of_Switches ~Group * domain * fluency_score)
summary(domain_switch_numb_lm)
car::Anova(domain_switch_numb_lm)
emmeans(domain_switch_numb_lm,pairwise~Group*domain,simple="Group")
#marginal intx of domain x fluency score
```

### Cluster Plots
```{r}
#cluster size
clusters<-emmip(domain_cluster_size_lm,Group ~ fluency_score | domain, cov.reduce = range, CI = TRUE) + 
  guides(color = guide_legend(title = NULL)) +
  theme_few() + facet_wrap(~domain)+
  scale_colour_paletteer_d("nationalparkcolors::CraterLake") +
  labs(
    x = "Fluency Score",
    y = "Predicted Average Cluster Size") +theme(aspect.ratio=.75,legend.position="right")

#number of switches
switches<-emmip(domain_switch_numb_lm, Group ~ fluency_score | domain, cov.reduce = range,CI=TRUE)+ 
      guides(color = guide_legend(title = NULL)) + facet_wrap(~domain)+ 
      theme_few() + scale_colour_paletteer_d("nationalparkcolors::CraterLake") +
  labs(
    x = "Fluency Score",
    y = "Predicted Number of Switches")+theme(aspect.ratio=.75)

clusters+switches+ plot_layout(ncol = 1)
```

## Within Cluster vs Switch Analyses
```{r}
#filtering to only include best for each indv
best_switches <- full_switch_data %>%
  filter(paste(Subject,Group,domain,full_method,dimension,structure,alpha) %in% paste(best_blended_delta_bic$Subject,best_blended_delta_bic$Group,best_blended_delta_bic$domain, best_blended_delta_bic$full_method,best_blended_delta_bic$dimension,best_blended_delta_bic$structure,best_blended_delta_bic$alpha)) %>%
  mutate(Switch_Value = if_else(Switch_Value == 2, 1, Switch_Value))

#Calculate item_no consistently in all_lexical to use in joining
items_lexical_data <- full_lexical_data %>%filter(paste(Subject,Group,domain,dimension,structure,alpha) %in% paste(best_blended_delta_bic$Subject,best_blended_delta_bic$Group,best_blended_delta_bic$domain,best_blended_delta_bic$dimension,best_blended_delta_bic$structure, best_blended_delta_bic$alpha))%>%select(-structure,-dimension,-alpha)%>%
  group_by(Subject, Group, domain) %>%
  mutate(row_id = row_number() - 1)  # Adjust row_id to start at 0 to account for first item

#this code drops the first item from each list, and if switch_value = 1, marked as a switch trial, if switch_value=1, marked as within cluster transition, also labels clusters and switches by ordinal position
#new first item only marked as within cluster if original switch_value==0
switches_recoded <- best_switches %>%
  group_by(Subject,Group,domain) %>%
  mutate(
    Cluster_Number = cumsum(Switch_Value == 1),
    Switch = case_when(
      row_number() == 1 ~ "Within Cluster Transition",  # Changed to "Within Cluster Transition"
      Cluster_Number != lag(Cluster_Number) ~ "Switch Trial", # Adding cluster number
      TRUE ~ "Within Cluster Transition"
    )
  ) %>%
  mutate(Switch_Number = case_when(
    Switch == "Switch Trial" ~ Cluster_Number - 1,  # Adding switch number
    TRUE ~ NA_real_
  )) %>%
  group_by(Subject,Group, domain) %>%
  mutate(row_id = row_number() - 1) %>%  # Adjusting row_id to start at 0 to be able to drop first item
  left_join(items_lexical_data, by = c("Subject","Group", "Fluency_Item", "domain","row_id")) %>%
  rename(
    "semantic" = "Semantic_Similarity",
    "phonological" = "Phonological_Similarity",
    "frequency" = "Frequency_Value"
  ) %>% filter(row_id!=0)%>% #dropping first word of each list
  pivot_longer(names_to = "cue", cols = c(semantic, phonological, frequency))%>%mutate(Group = factor(Group, levels = c("NH","CI"))) %>%left_join(just_fluency)%>%
  rename("fluency_score"="N")
                                                                                       

#save indv best model lexical values for network analysis
switches_wide<-switches_recoded %>%pivot_wider(names_from = cue, values_from = value)
write.csv(switches_wide,"best_indv_model_lexical.csv")             


## does cluster/switch designation predict semantic similarity, phonological similarity, and frequency for each Group x Domain
switch_results <- switches_recoded  %>%mutate(Group = factor(Group, levels = c("NH","CI")))%>%
  group_by(cue) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group*domain*Switch*fluency_score, data = .)) 
  )

#View results
#large so calling indp
print("Semantic")
switch_results$model[[1]] %>% summary() 
print("phonological")
switch_results$model[[2]] %>% summary()
print("Frequency")
switch_results$model[[3]] %>% summary()

#semantic <-sig effect of switch
#ns fx of phonology or frequency

#just looking at those with multimodal delta switch methods
switch_results_multi <- switches_recoded%>%filter(method=="multimodaldelta")%>%mutate(Group = factor(Group, levels = c("NH","CI")))%>%
  group_by(cue) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group*domain*Switch*fluency_score, data = .)) 
  )

#View results
#large so calling indp
print("Semantic")
switch_results_multi$model[[1]] %>% summary() 
print("phonological")
switch_results_multi$model[[2]] %>% summary()
print("Frequency")
switch_results_multi$model[[3]] %>% summary()

#emmeans(switch_results_multi$model[[2]],pairwise~domain*Group,simple="Group",pbkrtest.limit = 35657)

#semantic <-sig effect of switch, domain, sig intx with group x domain (sem sim higher in animals than in foods, only for NH)
#phonology<-sig effect of domain x switch (greater within cluster phon similarity for foods), marginal intx of group x domain (higher phon sim for CIs)
#ns freq


```


### Within/Across Cluster plots
```{r}
plots_switch<-switch_results %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ Switch | domain,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "", x = "Switch", y = paste("Estimated", unique(cue))) +facet_wrap(~domain) 
      +scale_colour_paletteer_d("nationalparkcolors::CraterLake")  +
      scale_fill_paletteer_d("nationalparkcolors::CraterLake") +
      theme_few() + 
      theme(legend.position = "top")))

plots_switch$emmip_plot[[1]]
plots_switch$emmip_plot[[2]]
plots_switch$emmip_plot[[3]]

#just multimodal methods
plots_switch_multi<-switch_results_multi %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ Switch | domain,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "", x = "Switch", y = paste("Estimated", unique(cue))) +facet_wrap(~domain) 
      +scale_colour_paletteer_d("nationalparkcolors::CraterLake")  +
      scale_fill_paletteer_d("nationalparkcolors::CraterLake") +
      theme_few() + 
      theme(legend.position = "top")))

plots_switch_multi$emmip_plot[[1]]
plots_switch_multi$emmip_plot[[2]]
plots_switch_multi$emmip_plot[[3]]

```
## Looking at correlation of sem x phon similarity

```{r}

#on an item level

sem_phon_model<-lm(data = switches_recoded_corr, semantic ~ phonological*Group*domain)
summary(sem_phon_model)
car::Anova(sem_phon_model)
#sig intx of phonology x domain
#almost sig (p=.05) intx of group x domain x phonology
```

### Sem x phon plot

```{r}
switches_recoded_corr<-switches_recoded %>% pivot_wider(names_from = cue, values_from = value)%>% select(Subject, Group, Fluency_Item,semantic, phonological, 
         domain) %>% na.omit() %>%mutate(Group = factor(Group, levels = c("NH","CI")))
  ggplot(data=switches_recoded_corr,aes(x = semantic, y = phonological, group = Group, color = Group)) + facet_wrap(~domain)+
  theme_few() +  
  geom_smooth(method = "lm", se = TRUE,size=1) +
  theme(
    aspect.ratio = 1,  
    axis.text.x = element_text( hjust = 1, face = "bold"))+scale_colour_paletteer_d("nationalparkcolors::CraterLake")+labs(y = "Phonological Similarity", x = "Semantic Similarity")

```

